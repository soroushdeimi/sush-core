#!/usr/bin/env python3
"""
Comprehensive Benchmark & Stress-Test Suite for sushCore
Measures real performance using actual implementations (no mocks).
"""

import asyncio
import csv
import logging
import os
import random
import secrets
import socket
import sys
import time
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

import numpy as np

from sush.client import ClientConfig, SushClient
from sush.control.adaptive_control import AdaptiveControlLoop
from sush.control.censorship_detector import CensorshipDetector, NetworkMetrics
from sush.control.response_engine import ResponseEngine
from sush.control.threat_monitor import ThreatMonitor
from sush.core.quantum_obfuscator import QuantumObfuscator
from sush.core.traffic_morphing import PaddingProfile, TrafficMorphingEngine
from sush.server import ServerConfig, SushServer
from sush.transport.adaptive_transport import AdaptiveTransport, TransportMode

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

DATA_DIR = Path("tests/data")
DATA_DIR.mkdir(parents=True, exist_ok=True)
CSV_FILE = DATA_DIR / "benchmark_results.csv"


class BenchmarkRunner:
    """Run comprehensive benchmarks on real sushCore components."""

    def __init__(self):
        self.results = []
        self.server = None
        self.client = None

    async def experiment_a_crypto_overhead(self):
        """Experiment A: Measure CPU-bound crypto processing overhead."""
        logger.info("=" * 60)
        logger.info("Experiment A: Crypto Overhead Analysis (CPU Bound)")
        logger.info("=" * 60)

        obfuscator = QuantumObfuscator()
        payload_sizes = [1024, 10240, 102400, 1048576]  # 1KB, 10KB, 100KB, 1MB
        iterations = 50

        for size in payload_sizes:
            logger.info(f"\nTesting payload size: {size} bytes ({size / 1024:.1f} KB)")

            # Generate peer keypair for session
            peer_pub, peer_priv = obfuscator.kem.generate_keypair()

            obfuscate_times = []
            deobfuscate_times = []
            total_times = []

            for iteration in range(iterations):
                payload = os.urandom(size)
                session_id = f"bench_a_{size}_{iteration}_{secrets.token_hex(4)}"

                # Initialize session
                await obfuscator.initialize_session(
                    session_id,
                    peer_pub,
                    threat_level=None,  # Use default
                    network_condition=None,
                )

                # Measure obfuscation
                start = time.perf_counter()
                obfuscated = await obfuscator.obfuscate_data(session_id, payload)
                obfuscate_time = time.perf_counter() - start

                # Measure deobfuscation
                start = time.perf_counter()
                deobfuscated = await obfuscator.deobfuscate_data(session_id, obfuscated)
                deobfuscate_time = time.perf_counter() - start

                total_time = obfuscate_time + deobfuscate_time

                # Verify correctness
                assert deobfuscated == payload, f"Data corruption at iteration {iteration}!"

                obfuscate_times.append(obfuscate_time)
                deobfuscate_times.append(deobfuscate_time)
                total_times.append(total_time)

                if (iteration + 1) % 10 == 0:
                    logger.info(f"  Completed {iteration + 1}/{iterations} iterations")

            # Calculate statistics
            avg_obfuscate = np.mean(obfuscate_times)
            avg_deobfuscate = np.mean(deobfuscate_times)
            avg_total = np.mean(total_times)
            std_total = np.std(total_times)

            # Calculate throughput (MB/s)
            throughput_mbps = (size * 8) / (avg_total * 1_000_000)

            self.results.append(
                {
                    "experiment": "A",
                    "payload_size_bytes": size,
                    "iterations": iterations,
                    "avg_obfuscate_time_ms": avg_obfuscate * 1000,
                    "avg_deobfuscate_time_ms": avg_deobfuscate * 1000,
                    "avg_total_time_ms": avg_total * 1000,
                    "std_total_time_ms": std_total * 1000,
                    "throughput_mbps": throughput_mbps,
                    "timestamp": time.time(),
                }
            )

            logger.info(
                f"  Results: Avg Total={avg_total * 1000:.2f}ms, "
                f"Std={std_total * 1000:.2f}ms, Throughput={throughput_mbps:.2f} Mbps"
            )

    async def experiment_b_end_to_end_throughput(self):
        """Experiment B: Measure end-to-end throughput with real server/client."""
        logger.info("\n" + "=" * 60)
        logger.info("Experiment B: End-to-End Throughput (IO Bound)")
        logger.info("=" * 60)

        # Use safe ephemeral ports (49152-65535 range) to avoid firewall/ISP blocking
        # These ports are in the IANA ephemeral port range and unlikely to be blocked
        # We use localhost (127.0.0.1) only, not 0.0.0.0, to ensure no external access
        safe_ports = [54321, 55555, 60000, 61000]  # Fallback options
        safe_address = "127.0.0.1"  # Only localhost, not 0.0.0.0

        # Find an available port
        safe_port = None
        for port in safe_ports:
            try:
                test_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                test_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                test_socket.bind((safe_address, port))
                test_socket.close()
                safe_port = port
                break
            except OSError:
                continue

        if safe_port is None:
            raise RuntimeError(f"None of the safe ports {safe_ports} are available")

        # Start server on safe port
        server_config = ServerConfig(
            node_id="benchmark_server",
            listen_address=safe_address,
            listen_ports=[safe_port],
        )
        self.server = SushServer(server_config)
        await self.server.start()
        logger.info(f"Server started on {safe_address}:{safe_port} (safe ephemeral port)")

        # Wait for server to be ready
        await asyncio.sleep(2.0)

        try:
            # Test both transport modes
            modes = [
                ("DIRECT", TransportMode.DIRECT),
                ("STEGANOGRAPHIC", TransportMode.STEGANOGRAPHIC),
            ]

            file_size = 10 * 1024 * 1024  # 10MB
            chunk_size = 64 * 1024  # 64KB chunks

            for mode_name, transport_mode in modes:
                logger.info(f"\nTesting {mode_name} mode...")

                # Create client with specific transport mode
                client_config = ClientConfig(
                    node_id="benchmark_client",
                )
                self.client = SushClient(client_config)

                # Configure transport mode
                await self.client.adaptive_transport.configure({"mode": transport_mode})

                await self.client.start()
                logger.info("Client started")

                # Generate test data
                test_data = os.urandom(file_size)
                total_bytes_sent = 0
                total_bytes_received = 0

                start_time = time.perf_counter()

                # Simulate file transfer through direct connection (bypass mirror_network for localhost)
                try:
                    # Use direct TCP connection for localhost testing
                    logger.info(f"Establishing direct TCP connection to 127.0.0.1:{safe_port}")
                    reader, writer = await asyncio.open_connection("127.0.0.1", safe_port)
                    logger.info("Direct connection established")

                    # Send data in chunks
                    for i in range(0, len(test_data), chunk_size):
                        chunk = test_data[i : i + chunk_size]
                        writer.write(chunk)
                        await writer.drain()
                        total_bytes_sent += len(chunk)

                        # Simulate network delay (localhost simulation)
                        await asyncio.sleep(random.uniform(0.01, 0.05))

                        # Try to receive response (if any)
                        try:
                            response = await asyncio.wait_for(reader.read(chunk_size), timeout=0.1)
                            if response:
                                total_bytes_received += len(response)
                        except asyncio.TimeoutError:
                            pass  # No response expected
                        except Exception:
                            pass  # Ignore other errors

                    # Close connection
                    writer.close()
                    await writer.wait_closed()
                    logger.info("Connection closed")

                except Exception as e:
                    logger.error(f"Error during transfer: {e}")
                    import traceback

                    logger.error(traceback.format_exc())

                elapsed_time = time.perf_counter() - start_time

                # Calculate throughput
                if elapsed_time > 0:
                    effective_throughput_mbps = (total_bytes_sent * 8) / (elapsed_time * 1_000_000)
                else:
                    effective_throughput_mbps = 0

                self.results.append(
                    {
                        "experiment": "B",
                        "mode": mode_name,
                        "file_size_bytes": file_size,
                        "bytes_sent": total_bytes_sent,
                        "bytes_received": total_bytes_received,
                        "elapsed_time_seconds": elapsed_time,
                        "effective_throughput_mbps": effective_throughput_mbps,
                        "timestamp": time.time(),
                    }
                )

                logger.info(
                    f"  {mode_name}: Sent {total_bytes_sent / 1024 / 1024:.2f} MB in "
                    f"{elapsed_time:.2f}s, Throughput: {effective_throughput_mbps:.2f} Mbps"
                )

                await self.client.stop()

        finally:
            # Cleanup
            if self.server:
                await self.server.stop()
                logger.info("Server stopped")

    async def experiment_c_adaptive_response_time(self):
        """Experiment C: Measure adaptive control loop response time."""
        logger.info("\n" + "=" * 60)
        logger.info("Experiment C: Adaptive Control Response Time")
        logger.info("=" * 60)

        # Initialize components
        detector = CensorshipDetector()
        monitor = ThreatMonitor()
        response_engine = ResponseEngine()
        control_loop = AdaptiveControlLoop(adaptation_interval=0.5)  # Faster for testing

        # Initialize with components
        await control_loop.initialize_components(
            censorship_detector=detector,
            threat_monitor=monitor,
            response_engine=response_engine,
            quantum_obfuscator=QuantumObfuscator(),
            adaptive_transport=AdaptiveTransport(),
            mirror_network=None,
        )

        await detector.start_monitoring()
        await monitor.start_monitoring()
        await response_engine.start()
        await control_loop.start()

        logger.info("Control loop started")

        try:
            # Phase 1: Normal metrics (0-3 seconds)
            logger.info("Phase 1: Injecting normal metrics (0-3s)...")
            normal_metrics = [
                NetworkMetrics(
                    timestamp=time.time() + i * 0.3,
                    latency=20.0 + random.uniform(-5, 5),
                    packet_loss=0.001 + random.uniform(0, 0.01),
                    throughput=5000000.0,  # 5 Mbps
                    connection_success_rate=0.99,
                    rst_packets=0,
                    retransmissions=0,
                    jitter=2.0 + random.uniform(0, 5),
                    bandwidth_utilization=0.3 + random.uniform(0, 0.2),
                )
                for i in range(10)
            ]

            initial_state = control_loop.get_system_status()["system_state"]
            logger.info(f"Initial system state: {initial_state}")

            for metrics in normal_metrics:
                await detector.record_metrics(metrics)
                await asyncio.sleep(0.3)

            # Phase 2: Attack injection at T=3s
            attack_start_time = time.perf_counter()
            logger.info("\nPhase 2: Injecting attack metrics (T=3s)...")

            attack_metrics = [
                NetworkMetrics(
                    timestamp=time.time() + i * 0.1,
                    latency=500.0 + random.uniform(0, 200),  # High latency
                    packet_loss=0.5 + random.uniform(0, 0.3),  # High loss
                    throughput=100000.0,  # Low throughput
                    connection_success_rate=0.1 + random.uniform(0, 0.2),  # Low success
                    rst_packets=int(np.random.exponential(20)),  # Many resets
                    retransmissions=int(np.random.exponential(50)),  # Many retransmissions
                    jitter=100.0 + random.uniform(0, 50),  # High jitter
                    bandwidth_utilization=0.9 + random.uniform(0, 0.1),  # High utilization
                )
                for i in range(50)  # More attack metrics for better detection
            ]

            adaptation_detected = False
            adaptation_time = None

            for _i, metrics in enumerate(attack_metrics):
                await detector.record_metrics(metrics)
                await asyncio.sleep(0.1)  # Faster injection

                # Check if system state changed
                current_status = control_loop.get_system_status()
                current_state = current_status["system_state"]

                if current_state != initial_state and not adaptation_detected:
                    adaptation_time = time.perf_counter() - attack_start_time
                    adaptation_detected = True
                    logger.info(
                        f"  âœ“ Adaptation detected! State changed from {initial_state} to {current_state} "
                        f"after {adaptation_time * 1000:.2f}ms"
                    )
                    break  # Stop checking once adaptation is detected

            if adaptation_detected:
                self.results.append(
                    {
                        "experiment": "C",
                        "initial_state": str(initial_state),
                        "final_state": str(current_state),
                        "reaction_time_ms": adaptation_time * 1000,
                        "attack_metrics_injected": len(attack_metrics),
                        "timestamp": time.time(),
                    }
                )
            else:
                logger.warning("No adaptation detected within observation period")
                self.results.append(
                    {
                        "experiment": "C",
                        "initial_state": str(initial_state),
                        "final_state": str(current_state),
                        "reaction_time_ms": None,
                        "attack_metrics_injected": len(attack_metrics),
                        "timestamp": time.time(),
                    }
                )

        finally:
            await control_loop.stop()
            await response_engine.stop()
            await monitor.stop_monitoring()
            await detector.stop_monitoring()
            logger.info("Control loop stopped")

    async def experiment_d_bandwidth_efficiency(self):
        """Experiment D: Measure bandwidth efficiency of adaptive padding strategies."""
        logger.info("=" * 60)
        logger.info("Experiment D: Bandwidth Efficiency Analysis")
        logger.info("=" * 60)

        morphing_engine = TrafficMorphingEngine()
        packet_size = 50  # Small packets (50 bytes)
        num_packets = 100

        strategies = [
            ("Direct", None, False),  # No padding
            ("Adaptive-Interactive", PaddingProfile.INTERACTIVE, True),
            ("Adaptive-Streaming", PaddingProfile.STREAMING, True),
            ("Adaptive-Paranoid", PaddingProfile.PARANOID, True),
        ]

        for strategy_name, padding_profile, enable_adaptive in strategies:
            logger.info(f"\nTesting strategy: {strategy_name}")

            # Configure morphing engine
            morphing_engine.enable_adaptive_padding(enable_adaptive)
            if padding_profile:
                morphing_engine.set_padding_profile(padding_profile)

            total_original_bytes = 0
            total_padded_bytes = 0
            overhead_bytes_list = []
            overhead_percent_list = []

            for i in range(num_packets):
                payload = os.urandom(packet_size)
                total_original_bytes += len(payload)

                # Apply padding
                if enable_adaptive:
                    morphed = morphing_engine.obfuscate_data(payload, padding_profile)
                else:
                    # Direct mode: minimal overhead (just 8-byte header, no extra padding)
                    morphed = morphing_engine._pad_to_size(payload, len(payload) + 8)

                total_padded_bytes += len(morphed)
                overhead = len(morphed) - len(payload)
                overhead_bytes_list.append(overhead)
                overhead_percent = (overhead / len(payload)) * 100 if len(payload) > 0 else 0
                overhead_percent_list.append(overhead_percent)

                if (i + 1) % 20 == 0:
                    logger.info(f"  Processed {i + 1}/{num_packets} packets")

            # Calculate statistics
            avg_overhead_bytes = np.mean(overhead_bytes_list)
            avg_overhead_percent = np.mean(overhead_percent_list)
            std_overhead_percent = np.std(overhead_percent_list)
            total_overhead_bytes = total_padded_bytes - total_original_bytes
            total_overhead_percent = (total_overhead_bytes / total_original_bytes) * 100

            logger.info(f"\n  Results for {strategy_name}:")
            logger.info(f"    Original bytes: {total_original_bytes:,}")
            logger.info(f"    Padded bytes: {total_padded_bytes:,}")
            logger.info(
                f"    Total overhead: {total_overhead_bytes:,} bytes ({total_overhead_percent:.2f}%)"
            )
            logger.info(
                f"    Avg overhead per packet: {avg_overhead_bytes:.2f} bytes ({avg_overhead_percent:.2f}%)"
            )
            logger.info(f"    Std dev: {std_overhead_percent:.2f}%")

            # Save result
            self.results.append(
                {
                    "experiment": "D",
                    "strategy": strategy_name,
                    "packet_size_bytes": packet_size,
                    "num_packets": num_packets,
                    "total_original_bytes": total_original_bytes,
                    "total_padded_bytes": total_padded_bytes,
                    "total_overhead_bytes": total_overhead_bytes,
                    "total_overhead_percent": total_overhead_percent,
                    "avg_overhead_bytes": avg_overhead_bytes,
                    "avg_overhead_percent": avg_overhead_percent,
                    "std_overhead_percent": std_overhead_percent,
                    "timestamp": time.time(),
                }
            )

        logger.info("\n" + "=" * 60)
        logger.info("Experiment D completed")
        logger.info("=" * 60)

    async def experiment_d2_bandwidth_efficiency_large_packets(self):
        """Experiment D2: Measure bandwidth efficiency for large packets (1000 bytes)."""
        logger.info("=" * 60)
        logger.info("Experiment D2: Bandwidth Efficiency Analysis (Large Packets)")
        logger.info("=" * 60)

        morphing_engine = TrafficMorphingEngine()
        packet_size = 1000  # Large packets (1000 bytes)
        num_packets = 100

        strategies = [
            ("Direct", None, False),  # No padding
            ("Adaptive-Interactive", PaddingProfile.INTERACTIVE, True),
            ("Adaptive-Streaming", PaddingProfile.STREAMING, True),
            ("Adaptive-Paranoid", PaddingProfile.PARANOID, True),
        ]

        for strategy_name, padding_profile, enable_adaptive in strategies:
            logger.info(f"\nTesting strategy: {strategy_name}")

            # Configure morphing engine
            morphing_engine.enable_adaptive_padding(enable_adaptive)
            if padding_profile:
                morphing_engine.set_padding_profile(padding_profile)

            total_original_bytes = 0
            total_padded_bytes = 0
            overhead_bytes_list = []
            overhead_percent_list = []

            for i in range(num_packets):
                payload = os.urandom(packet_size)
                total_original_bytes += len(payload)

                # Apply padding
                if enable_adaptive:
                    morphed = morphing_engine.obfuscate_data(payload, padding_profile)
                else:
                    # Direct mode: minimal overhead (just 8-byte header, no extra padding)
                    morphed = morphing_engine._pad_to_size(payload, len(payload) + 8)

                total_padded_bytes += len(morphed)
                overhead = len(morphed) - len(payload)
                overhead_bytes_list.append(overhead)
                overhead_percent = (overhead / len(payload)) * 100 if len(payload) > 0 else 0
                overhead_percent_list.append(overhead_percent)

                if (i + 1) % 20 == 0:
                    logger.info(f"  Processed {i + 1}/{num_packets} packets")

            # Calculate statistics
            avg_overhead_bytes = np.mean(overhead_bytes_list)
            avg_overhead_percent = np.mean(overhead_percent_list)
            std_overhead_percent = np.std(overhead_percent_list)
            total_overhead_bytes = total_padded_bytes - total_original_bytes
            total_overhead_percent = (total_overhead_bytes / total_original_bytes) * 100

            logger.info(f"\n  Results for {strategy_name}:")
            logger.info(f"    Original bytes: {total_original_bytes:,}")
            logger.info(f"    Padded bytes: {total_padded_bytes:,}")
            logger.info(
                f"    Total overhead: {total_overhead_bytes:,} bytes ({total_overhead_percent:.2f}%)"
            )
            logger.info(
                f"    Avg overhead per packet: {avg_overhead_bytes:.2f} bytes ({avg_overhead_percent:.2f}%)"
            )
            logger.info(f"    Std dev: {std_overhead_percent:.2f}%")

            # Save result
            self.results.append(
                {
                    "experiment": "D2",
                    "strategy": strategy_name,
                    "packet_size_bytes": packet_size,
                    "num_packets": num_packets,
                    "total_original_bytes": total_original_bytes,
                    "total_padded_bytes": total_padded_bytes,
                    "total_overhead_bytes": total_overhead_bytes,
                    "total_overhead_percent": total_overhead_percent,
                    "avg_overhead_bytes": avg_overhead_bytes,
                    "avg_overhead_percent": avg_overhead_percent,
                    "std_overhead_percent": std_overhead_percent,
                    "timestamp": time.time(),
                }
            )

        logger.info("\n" + "=" * 60)
        logger.info("Experiment D2 completed")
        logger.info("=" * 60)

    def save_results(self):
        """Save all results to CSV file."""
        if not self.results:
            logger.warning("No results to save")
            return

        # Get all unique fieldnames
        fieldnames = set()
        for result in self.results:
            fieldnames.update(result.keys())

        fieldnames = sorted(fieldnames)

        with open(CSV_FILE, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(self.results)

        logger.info(f"\n{'=' * 60}")
        logger.info(f"Saved {len(self.results)} results to {CSV_FILE}")
        logger.info(f"{'=' * 60}")


async def main():
    """Run all benchmark experiments."""
    print("=" * 60)
    print("sushCore Comprehensive Benchmark Suite")
    print("=" * 60)
    print(f"Results will be saved to: {CSV_FILE}")
    print("=" * 60)

    runner = BenchmarkRunner()

    try:
        # Run all experiments
        await runner.experiment_a_crypto_overhead()
        await runner.experiment_b_end_to_end_throughput()
        await runner.experiment_c_adaptive_response_time()
        await runner.experiment_d_bandwidth_efficiency()
        await runner.experiment_d2_bandwidth_efficiency_large_packets()

        # Save results
        runner.save_results()

        print("\n" + "=" * 60)
        print("All benchmarks completed successfully!")
        print(f"Results saved to: {CSV_FILE}")
        print("=" * 60)

    except Exception as e:
        logger.error(f"Benchmark failed: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    asyncio.run(main())
